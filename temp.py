# train.py

import pandas as pd
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")

# --- –ü—É—Ç–∏ ---
DATASET_PATH = "src/data/faq_full.parquet"
OUTPUT_DIR = "src/models/finetuned_rugpt"

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
df = pd.read_parquet(DATASET_PATH)  # –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ pd.read_json(...) –µ—Å–ª–∏ –Ω—É–∂–Ω–æ


# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤
def format_example(row):
    return f"–í–æ–ø—Ä–æ—Å: {row['user_question']}\n–û—Ç–≤–µ—Ç: {row['assistant_answer']}</s>"


df["text"] = df.apply(format_example, axis=1)
dataset = Dataset.from_pandas(df[["text"]])

# --- –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ---
MODEL_NAME = "ai-forever/rugpt3small_based_on_gpt2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)


# --- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ---
def tokenize_function(examples):
    return tokenizer(
        examples["text"], truncation=True, max_length=512, padding="max_length"
    )


tokenized_datasets = dataset.map(
    tokenize_function, batched=True, num_proc=4, remove_columns=["text"]
)

# --- Data collator ---
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è ---
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    logging_dir="logs",
    logging_steps=100,
    fp16=torch.cuda.is_available(),
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    report_to="none",
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets,
    tokenizer=tokenizer,
)

# --- –û–±—É—á–µ–Ω–∏–µ ---
print("üß† –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
trainer.train()

# --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---
print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å –≤ {OUTPUT_DIR}...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
